import openai
import pandas as pd
import numpy as np
import time
import os
import csv
import requests
import json
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
from requests.exceptions import RequestException

# âœ… DeepSeek é…ç½®
DEEPSEEK_API_BASE = "https://api.probex.top/v1"  # DeepSeek APIåœ°å€
DEEPSEEK_API_KEY = ""  # æ›¿æ¢ä¸ºä½ çš„DeepSeek APIå¯†é’¥

# é…ç½®å‚æ•°
BATCH_SIZE = 500  # æ¯æ‰¹æ¬¡å¤„ç†çš„æ•°æ®é‡
MAX_RETRIES = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
INITIAL_DELAY = 1  # åˆå§‹é‡è¯•å»¶è¿Ÿ(ç§’)
BACKOFF_FACTOR = 2  # æŒ‡æ•°é€€é¿å› å­
RESULT_CSV = "deepseek_test_predictions.csv"  # ä½¿ç”¨CSVæ ¼å¼ä¿å­˜ç»“æœ
LOG_FILE = "processing_log.txt"
PROGRESS_FILE = "progress_checkpoint.txt"
MODEL_NAME = "deepseek-v3"  # DeepSeekæ¨¡å‹åç§°

# åŠ è½½æ•°æ®
train_df = pd.read_excel("train-new.xlsx", header=None, names=["text", "label"])
test_df = pd.read_excel("test-new.xlsx", header=None, names=["text", "label"])

# æ·»åŠ ç´¢å¼•åˆ—ï¼Œä¾¿äºè·Ÿè¸ªè¿›åº¦
test_df = test_df.reset_index(drop=False).rename(columns={'index': 'orig_index'})


# æ„é€ few-shotç¤ºä¾‹ï¼Œè‡ªåŠ¨é€‚åº”ä¸åŒçš„æ•°æ®é›†æ ‡ç­¾
def create_few_shot_messages(train_df, num_samples_per_label=10):
    # è·å–æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ‡ç­¾
    unique_labels = train_df['label'].unique()

    # æ„é€ ç³»ç»Ÿæç¤º
    system_prompt = (
            "è¯·å¯¹ä»¥ä¸‹çŸ­æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç±»åˆ«åŒ…æ‹¬ï¼š"
            + ", ".join([str(label) for label in unique_labels]) + "ã€‚\n"
                                                                   "æ³¨æ„ï¼šè¯·ä»…è¿”å›ç±»åˆ«æ ‡ç­¾ï¼Œä¸è¦åŒ…å«å…¶ä»–å­—ç¬¦æˆ–è§£é‡Šã€‚\n"
    )

    # æ„é€  few-shot ç¤ºä¾‹
    few_shot_messages = [{"role": "system", "content": system_prompt}]

    # ä¸ºæ¯ä¸ªæ ‡ç­¾é€‰æ‹©è‹¥å¹²ä¸ªç¤ºä¾‹
    examples = train_df.groupby("label").apply(
        lambda x: x.sample(n=num_samples_per_label, random_state=42)).reset_index(drop=True)

    for _, row in examples.iterrows():
        few_shot_messages.append({"role": "user", "content": row['text']})
        few_shot_messages.append({"role": "assistant", "content": str(row['label'])})  # ç¡®ä¿æ ‡ç­¾æ˜¯å­—ç¬¦ä¸²

    return few_shot_messages


# ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°æ„é€ æ¶ˆæ¯
few_shot_messages = create_few_shot_messages(train_df, num_samples_per_label=10)


# DeepSeek APIè°ƒç”¨å‡½æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
def deepseek_api_call(messages):
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.0,
        "max_tokens": 10,
        "stream": False
    }

    try:
        response = requests.post(
            f"{DEEPSEEK_API_BASE}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
        return response.json()
    except RequestException as e:
        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")


# å¢å¼ºçš„åˆ†ç±»å‡½æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
def classify_with_retry(text, few_shot_messages):
    messages = few_shot_messages + [{"role": "user", "content": text}]

    for attempt in range(MAX_RETRIES):
        try:
            # ä½¿ç”¨DeepSeek API
            response = deepseek_api_call(messages)

            # è·å–å›å¤å†…å®¹å¹¶æ¸…ç†
            reply = response['choices'][0]['message']['content'].strip()
            return reply

        except Exception as e:
            error_msg = str(e)

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•æ„Ÿå†…å®¹é”™è¯¯
            if "æ•æ„Ÿå†…å®¹" in error_msg or "ä¸å®‰å…¨" in error_msg or "safety" in error_msg.lower():
                print(f"\nğŸš« Content safety violation for record: {error_msg[:150]}")
                return "violation_error"

            # æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯
            elif "rate limit" in error_msg.lower() or "too many" in error_msg.lower():
                delay = INITIAL_DELAY * (BACKOFF_FACTOR ** attempt)
                print(f"\nâš ï¸ Rate limit exceeded (Attempt {attempt + 1}/{MAX_RETRIES}): {error_msg[:150]}...")
                print(f"ğŸ•’ Retrying in {delay} seconds...")
                time.sleep(delay)

            # å…¶ä»–å¯é‡è¯•é”™è¯¯
            elif any(keyword in error_msg.lower() for keyword in ["timeout", "connection", "server", "api"]):
                delay = INITIAL_DELAY * (BACKOFF_FACTOR ** attempt)
                print(f"\nâš ï¸ Attempt {attempt + 1}/{MAX_RETRIES} failed: {error_msg[:150]}...")
                print(f"ğŸ•’ Retrying in {delay} seconds...")
                time.sleep(delay)

            # ä¸å¯æ¢å¤çš„é”™è¯¯
            else:
                print(f"\nâŒ Unrecoverable error: {error_msg[:150]}")
                return "error"

    print(f"ğŸš¨ Request failed after {MAX_RETRIES} attempts")
    return "error"


# åˆå§‹åŒ–ç»“æœCSVæ–‡ä»¶
def init_csv_file():
    if not os.path.exists(RESULT_CSV):
        try:
            with open(RESULT_CSV, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['orig_index', 'text', 'label', 'pred', 'timestamp']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
            print(f"âœ… Created new result file: {RESULT_CSV}")
        except PermissionError as e:
            print(f"âŒ Permission denied when creating file: {e}")
            print("âš ï¸ Please close any programs that may be using this file and restart the script")
            exit(1)
    else:
        print(f"âœ… Using existing result file: {RESULT_CSV}")


# ä¿å­˜å•æ¡ç»“æœåˆ°CSV
def save_single_result(orig_index, text, label, pred):
    max_retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay = 1  # é‡è¯•å»¶è¿Ÿ(ç§’)

    for attempt in range(max_retries):
        try:
            # å°è¯•æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥
            with open(RESULT_CSV, 'a', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['orig_index', 'text', 'label', 'pred', 'timestamp']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writerow({
                    'orig_index': orig_index,
                    'text': text,
                    'label': label,
                    'pred': pred,
                    'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
                })
            return  # å†™å…¥æˆåŠŸï¼Œé€€å‡ºå‡½æ•°

        except PermissionError as e:
            # æƒé™é”™è¯¯å¤„ç†
            if attempt < max_retries - 1:
                print(
                    f"âš ï¸ Permission denied when saving record {orig_index}. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
            else:
                print(f"âŒ Failed to save record {orig_index} after {max_retries} attempts: {e}")
                # è®°å½•åˆ°é”™è¯¯æ—¥å¿—
                with open("save_errors.log", "a") as error_log:
                    error_log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Failed to save record {orig_index}: {e}\n")

        except Exception as e:
            print(f"âŒ Unexpected error when saving record {orig_index}: {e}")
            # è®°å½•åˆ°é”™è¯¯æ—¥å¿—
            with open("save_errors.log", "a") as error_log:
                error_log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Error saving record {orig_index}: {e}\n")
            break  # éæƒé™é”™è¯¯ï¼Œç«‹å³é€€å‡º


# è·å–å·²å¤„ç†çš„æ•°æ®ç´¢å¼•
def get_processed_indices():
    processed_indices = set()
    if os.path.exists(RESULT_CSV):
        try:
            with open(RESULT_CSV, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if row['orig_index'] and row['orig_index'].isdigit():
                        processed_indices.add(int(row['orig_index']))
        except Exception as e:
            print(f"âš ï¸ Error reading result file: {e}")
    return processed_indices


# ä¿å­˜è¿›åº¦æ£€æŸ¥ç‚¹
def save_progress_checkpoint(last_index):
    try:
        with open(PROGRESS_FILE, 'w') as f:
            f.write(str(last_index))
    except Exception as e:
        print(f"âš ï¸ Error saving progress checkpoint: {e}")


# åŠ è½½è¿›åº¦æ£€æŸ¥ç‚¹
def load_progress_checkpoint():
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r') as f:
                return int(f.read().strip())
        except:
            return 0
    return 0


# åˆå§‹åŒ–ç»“æœæ–‡ä»¶
init_csv_file()

# è·å–å·²å¤„ç†çš„æ•°æ®ç´¢å¼•
processed_indices = get_processed_indices()
print(f"â„¹ï¸ Found {len(processed_indices)} processed records")

# åŠ è½½è¿›åº¦æ£€æŸ¥ç‚¹
start_index = load_progress_checkpoint()
print(f"â„¹ï¸ Starting from index: {start_index}")

# åˆ†æ‰¹å¤„ç†æ•°æ®
total_samples = len(test_df)
num_batches = int(np.ceil(total_samples / BATCH_SIZE))

print(f"\nğŸš€ Starting processing of {total_samples} records")
print(f"ğŸ“¦ Batch size: {BATCH_SIZE}, Total batches: {num_batches}")
print(f"ğŸ§  Using model: {MODEL_NAME}")

# åˆ›å»ºæ€»è¿›åº¦æ¡
pbar_total = tqdm(total=total_samples, desc="Overall Progress", position=0)

# è®¾ç½®å·²å¤„ç†çš„è¿›åº¦
pbar_total.update(len(processed_indices))

for batch_idx in range(int(np.ceil(start_index / BATCH_SIZE)), num_batches):
    batch_start = batch_idx * BATCH_SIZE
    batch_end = min((batch_idx + 1) * BATCH_SIZE, total_samples)

    # åˆ›å»ºæ‰¹æ¬¡è¿›åº¦æ¡
    pbar_batch = tqdm(total=batch_end - batch_start, desc=f"Batch {batch_idx + 1}/{num_batches}", position=1)

    print(f"\nğŸ”§ Processing batch {batch_idx + 1}/{num_batches} (records {batch_start}-{batch_end - 1})")

    # å¤„ç†å½“å‰æ‰¹æ¬¡
    for idx in range(batch_start, batch_end):
        # è·³è¿‡å·²å¤„ç†çš„è®°å½•
        if idx in processed_indices:
            pbar_batch.update(1)
            pbar_total.update(1)
            continue

        row = test_df.iloc[idx]
        orig_index = row['orig_index']
        text = row['text']
        label = row['label']

        # è¿›è¡Œåˆ†ç±»
        pred = classify_with_retry(text, few_shot_messages)

        # å¦‚æœé¢„æµ‹æ˜¯"error"æˆ–"violation_error"ï¼Œä¿å­˜ä¸º"error"
        if pred in ["error", "violation_error"]:
            print(f"âš ï¸ Sensitive content or error detected for record {orig_index}. Saving 'error'.")
            pred = "error"

        # ä¿å­˜ç»“æœ
        save_single_result(orig_index, text, label, pred)

        # æ›´æ–°è¿›åº¦
        pbar_batch.update(1)
        pbar_total.update(1)

        # æ›´æ–°æ£€æŸ¥ç‚¹ï¼ˆæ¯10æ¡ä¿å­˜ä¸€æ¬¡ï¼‰
        if idx % 10 == 0:
            save_progress_checkpoint(idx)

    # å…³é—­æ‰¹æ¬¡è¿›åº¦æ¡
    pbar_batch.close()

    # ä¿å­˜æ‰¹æ¬¡ç»“æŸæ£€æŸ¥ç‚¹
    save_progress_checkpoint(batch_end)

    # è®°å½•è¿›åº¦
    try:
        with open(LOG_FILE, 'a') as f:
            f.write(f"Batch {batch_idx + 1}: Processed {batch_start}-{batch_end - 1} at {time.ctime()}\n")
    except Exception as e:
        print(f"âš ï¸ Error writing to log file: {e}")

    print(f"ğŸ’¾ Saved results for batch {batch_idx + 1}")

# å…³é—­æ€»è¿›åº¦æ¡
pbar_total.close()

# åˆ é™¤è¿›åº¦æ£€æŸ¥ç‚¹æ–‡ä»¶
if os.path.exists(PROGRESS_FILE):
    try:
        os.remove(PROGRESS_FILE)
        print("âœ… Removed progress checkpoint file")
    except Exception as e:
        print(f"âš ï¸ Error removing progress file: {e}")

# æœ€ç»ˆè¯„ä¼°
print("\nğŸ“Š All data processed, starting evaluation...")

# åŠ è½½å®Œæ•´ç»“æœ
try:
    result_df = pd.read_csv(RESULT_CSV)
except FileNotFoundError:
    print("âŒ Result file not found. Evaluation skipped.")
    exit(1)

# è¿‡æ»¤æ‰é”™è¯¯ç»“æœ
valid_df = result_df[(result_df['pred'] != 'error') & (result_df['pred'] != 'violation_error')]
error_count = (result_df['pred'] == 'error').sum()
violation_count = (result_df['pred'] == 'violation_error').sum()

if len(valid_df) > 0:
    # ç¡®ä¿æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
    y_true = valid_df["label"].astype(str).str.strip().str.lower()
    y_pred = valid_df["pred"].astype(str).str.strip().str.lower()

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ï¼ˆç¡®ä¿æ•°å­—æ ‡ç­¾å’Œå­—ç¬¦ä¸²æ ‡ç­¾èƒ½æ­£ç¡®åŒ¹é…ï¼‰
    unique_labels = sorted(set(y_true) | set(y_pred))

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="macro")

    print("\nğŸ¯ Final Evaluation Results:")
    print(f"âœ… Valid samples: {len(valid_df)}/{len(result_df)}")
    print(f"âŒ Error/failed samples: {error_count}")
    print(f"ğŸš« Sensitive content violations: {violation_count}")
    print("ğŸ¯ Accuracy:", acc)
    print("ğŸ¯ Macro F1 Score:", f1)
    print("\nğŸ§¾ Classification Report:")
    print(classification_report(y_true, y_pred, labels=unique_labels))

    # ä¿å­˜æœ€ç»ˆç»“æœåˆ°Excel
    try:
        result_file = f"{MODEL_NAME.replace('-', '_')}_predictions_final.xlsx"
        result_df.to_excel(result_file, index=False)
        print(f"\nâœ… Final results saved to {result_file}")
    except Exception as e:
        print(f"âŒ Error saving final results: {e}")
else:
    print("âŒ No valid results available for evaluation")

# ä¿å­˜è¯¦ç»†æŠ¥å‘Š
detailed_report = f"{MODEL_NAME.replace('-', '_')}_classification_report.txt"
try:
    with open(detailed_report, 'w') as f:
        f.write(f"Dataset size: {len(result_df)}\n")
        f.write(f"Valid samples: {len(valid_df)}\n")
        f.write(f"Error samples: {error_count}\n")
        f.write(f"Sensitive content violations: {violation_count}\n\n")
        if len(valid_df) > 0:
            f.write("Classification Report:\n")
            f.write(classification_report(y_true, y_pred, labels=unique_labels))
    print(f"ğŸ“ Detailed report saved to {detailed_report}")
except Exception as e:
    print(f"âŒ Error saving detailed report: {e}")

print("\nâœ… Processing complete!")