import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
import os


def calculate_metrics(file_path):
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
    file_ext = os.path.splitext(file_path)[1].lower()

    try:
        if file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path, header=None)
        elif file_ext == '.csv':
            # å°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                    # å¸¸è§åˆ†éš”ç¬¦æ£€æµ‹
                    if ';' in first_line:
                        sep = ';'
                    elif '\t' in first_line:
                        sep = '\t'
                    elif '|' in first_line:
                        sep = '|'
                    else:
                        sep = ','  # é»˜è®¤é€—å·åˆ†éš”
            except:
                sep = ','  # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤é€—å·åˆ†éš”

            df = pd.read_csv(file_path, header=None, sep=sep, encoding='utf-8', engine='python')
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            return
    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return

    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = {2: 'çœŸå®æ ‡ç­¾', 3: 'é¢„æµ‹æ ‡ç­¾'}
    if len(df.columns) < 4:
        print(f"âŒ æ–‡ä»¶åˆ—æ•°ä¸è¶³: éœ€è¦è‡³å°‘4åˆ—ï¼Œå®é™…åªæœ‰{len(df.columns)}åˆ—")
        return

    # é‡å‘½ååˆ—ä»¥ä¾¿å¤„ç†
    column_mapping = {}
    for idx, name in required_columns.items():
        if idx < len(df.columns):
            column_mapping[df.columns[idx]] = name
        else:
            print(f"âŒ ç¼ºå°‘ç¬¬{idx + 1}åˆ—")
            return

    df = df.rename(columns=column_mapping)

    # è¿‡æ»¤æ— æ•ˆè¡Œ
    initial_count = len(df)
    df = df.dropna(subset=['çœŸå®æ ‡ç­¾', 'é¢„æµ‹æ ‡ç­¾'])
    df = df[df['çœŸå®æ ‡ç­¾'].notna() & df['é¢„æµ‹æ ‡ç­¾'].notna()]

    if len(df) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ç”¨äºè®¡ç®—")
        return

    # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è¡Œæ•°
    filtered_count = initial_count - len(df)

    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹ä»¥ç¡®ä¿ä¸€è‡´æ€§
    df['çœŸå®æ ‡ç­¾'] = df['çœŸå®æ ‡ç­¾'].astype(str).str.strip().str.lower()
    df['é¢„æµ‹æ ‡ç­¾'] = df['é¢„æµ‹æ ‡ç­¾'].astype(str).str.strip().str.lower()

    # è·å–æ‰€æœ‰çœŸå®æ ‡ç­¾çš„ç±»åˆ«
    true_labels = df['çœŸå®æ ‡ç­¾'].unique()

    # è¯†åˆ«ä¸åœ¨çœŸå®æ ‡ç­¾ç±»åˆ«ä¸­çš„é¢„æµ‹
    invalid_mask = ~df['é¢„æµ‹æ ‡ç­¾'].isin(true_labels)
    invalid_count = invalid_mask.sum()

    # å¤„ç†æ— æ•ˆé¢„æµ‹ï¼šè®¾ä¸ºç‰¹æ®Šå€¼"invalid"
    df.loc[invalid_mask, 'é¢„æµ‹æ ‡ç­¾'] = 'invalid'

    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåªè€ƒè™‘æœ‰æ•ˆé¢„æµ‹ï¼‰
    valid_mask = ~invalid_mask
    valid_accuracy = accuracy_score(
        df.loc[valid_mask, 'çœŸå®æ ‡ç­¾'],
        df.loc[valid_mask, 'é¢„æµ‹æ ‡ç­¾']
    ) if valid_mask.any() else 0

    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡ï¼ˆåŒ…æ‹¬æ— æ•ˆé¢„æµ‹ï¼‰
    overall_accuracy = accuracy_score(
        df['çœŸå®æ ‡ç­¾'],
        df['é¢„æµ‹æ ‡ç­¾'].where(~invalid_mask, 'invalid')
    )

    # è®¡ç®—F1åˆ†æ•°ï¼ˆå®å¹³å‡å’Œå¾®å¹³å‡ï¼‰
    # åªè€ƒè™‘çœŸå®æ ‡ç­¾ä¸­å­˜åœ¨çš„ç±»åˆ«
    labels = [label for label in true_labels if label != 'invalid']

    macro_f1 = f1_score(
        df['çœŸå®æ ‡ç­¾'],
        df['é¢„æµ‹æ ‡ç­¾'],
        average='macro',
        labels=labels,
        zero_division=0
    )

    micro_f1 = f1_score(
        df['çœŸå®æ ‡ç­¾'],
        df['é¢„æµ‹æ ‡ç­¾'],
        average='micro',
        labels=labels,
        zero_division=0
    )

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(
        df['çœŸå®æ ‡ç­¾'],
        df['é¢„æµ‹æ ‡ç­¾'],
        labels=labels,
        zero_division=0,
        output_dict=True
    )

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æ•°æ®é›†åˆ†æ: {file_path}")
    print("=" * 50)
    print(f"ğŸ“ æ€»æ ·æœ¬æ•°: {initial_count}")
    print(f"ğŸš« è¢«è¿‡æ»¤çš„æ— æ•ˆæ ·æœ¬: {filtered_count}")
    print(f"âœ… æœ‰æ•ˆæ ·æœ¬æ•°: {len(df)}")
    print(f"âš ï¸ é¢„æµ‹æ ‡ç­¾ä¸åœ¨çœŸå®ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°: {invalid_count}")
    print("\n" + "-" * 50)
    print(f"ğŸ¯ å‡†ç¡®ç‡ (ä»…æœ‰æ•ˆé¢„æµ‹): {valid_accuracy:.4f}")
    print(f"ğŸ¯ æ•´ä½“å‡†ç¡®ç‡ (åŒ…å«æ— æ•ˆé¢„æµ‹): {overall_accuracy:.4f}")
    print(f"ğŸ¯ å®å¹³å‡F1åˆ†æ•°: {macro_f1:.4f}")
    print(f"ğŸ¯ å¾®å¹³å‡F1åˆ†æ•°: {micro_f1:.4f}")
    print("\n" + "-" * 50)
    print("ğŸ“ˆ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        df['çœŸå®æ ‡ç­¾'],
        df['é¢„æµ‹æ ‡ç­¾'],
        labels=labels,
        zero_division=0
    ))

    # ä¿å­˜è¯¦ç»†ç»“æœ
    result_df = pd.DataFrame({
        'çœŸå®æ ‡ç­¾': df['çœŸå®æ ‡ç­¾'],
        'é¢„æµ‹æ ‡ç­¾': df['é¢„æµ‹æ ‡ç­¾'],
        'æ˜¯å¦æ­£ç¡®': df['çœŸå®æ ‡ç­¾'] == df['é¢„æµ‹æ ‡ç­¾'],
        'æ˜¯å¦æœ‰æ•ˆé¢„æµ‹': ~invalid_mask
    })

    # æ ¹æ®è¾“å…¥æ–‡ä»¶ç±»å‹å†³å®šè¾“å‡ºæ ¼å¼
    output_file = file_path.replace(file_ext, '_analysis.xlsx')
    result_df.to_excel(output_file, index=False)
    print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return {
        'total_samples': initial_count,
        'filtered_samples': filtered_count,
        'valid_samples': len(df),
        'invalid_predictions': invalid_count,
        'valid_accuracy': valid_accuracy,
        'overall_accuracy': overall_accuracy,
        'macro_f1': macro_f1,
        'micro_f1': micro_f1,
        'classification_report': report
    }


# ===================== ä½¿ç”¨ç¤ºä¾‹ =====================
if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
    file_path = "gpt35_test_predictions.csv"

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(file_path)

    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨è¿™é‡Œè®¿é—®å…·ä½“çš„æŒ‡æ ‡å€¼
    if metrics:
        print("\n" + "=" * 50)
        print(f"å®å¹³å‡F1åˆ†æ•°: {metrics['macro_f1']:.4f}")
        print(f"æ•´ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}")